{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "J9xNro5cUWEn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "AffKjioAUYbD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importando Librarías"
      ],
      "metadata": {
        "id": "_2NjqQuHUis4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "HIH0br2TUadT",
        "outputId": "517d4951-d533-43ec-f113-622dbb84b9b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cf8a2f4fd30>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b351f92b113d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Path y Estructura de archivos .csv"
      ],
      "metadata": {
        "id": "axi9FVgqUm1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necesario definir si deseamos ingestas más archivos csv, solo es necesario modificar esto y el proceso estará preparado para más archivos"
      ],
      "metadata": {
        "id": "k2MPaH_vUxk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importante si deseamos lecturar más archivos .csv es importante solo configurar este archivo\n",
        "\n",
        "# Ruta de los archivos CSV\n",
        "file_paths = [\"country_wise_latest.csv\",\n",
        "              \"covid_19_clean_complete.csv\",\n",
        "              \"day_wise.csv\",\n",
        "              \"full_grouped.csv\",\n",
        "              \"usa_county_wise.csv\",\n",
        "              \"worldometer_data.csv\"]\n",
        "\n",
        "# Definir esquemas para cada archivo CSV\n",
        "schema = {\n",
        "    \"country_wise_latest.csv\": StructType([\n",
        "        StructField(\"country_region\", StringType(), True),\n",
        "        StructField(\"confirmed\", IntegerType(), False),\n",
        "        StructField(\"deaths\", IntegerType(), False),\n",
        "        StructField(\"recovered\", IntegerType(), False),\n",
        "        StructField(\"active\", IntegerType(), False),\n",
        "        StructField(\"new_cases\", IntegerType(), False),\n",
        "        StructField(\"new_deaths\", IntegerType(), False),\n",
        "        StructField(\"new_recovered\", IntegerType(), False),\n",
        "        StructField(\"deaths_100_cases\", DoubleType(), False),\n",
        "        StructField(\"recovered_100_cases\", DoubleType(), False),\n",
        "        StructField(\"deaths_100_recovered\", DoubleType(), False),\n",
        "        StructField(\"confirmed_last_week\", IntegerType(), False),\n",
        "        StructField(\"one_week_change\", IntegerType(), False),\n",
        "        StructField(\"one_week_percentage_increase\", DoubleType(), False),\n",
        "        StructField(\"who_region\", StringType(), True)\n",
        "    ]),\n",
        "        \"covid_19_clean_complete.csv\": StructType([\n",
        "        StructField(\"province_state\", StringType(), True),\n",
        "        StructField(\"country_region\", StringType(), True),\n",
        "        StructField(\"lat\", DoubleType(), True),\n",
        "        StructField(\"long\", DoubleType(), True),\n",
        "        StructField(\"date\", DateType(), True),\n",
        "        StructField(\"confirmed\", IntegerType(), True),\n",
        "        StructField(\"deaths\", IntegerType(), True),\n",
        "        StructField(\"recovered\", IntegerType(), True),\n",
        "        StructField(\"active\", IntegerType(), True),\n",
        "        StructField(\"who_region\", StringType(), True)\n",
        "    ]),\n",
        "    \"day_wise.csv\": StructType([\n",
        "        StructField(\"date\", DateType(), True),\n",
        "        StructField(\"confirmed\", IntegerType(), True),\n",
        "        StructField(\"deaths\", IntegerType(), True),\n",
        "        StructField(\"recovered\", IntegerType(), True),\n",
        "        StructField(\"active\", IntegerType(), True),\n",
        "        StructField(\"new_cases\", IntegerType(), True),\n",
        "        StructField(\"new_deaths\", IntegerType(), True),\n",
        "        StructField(\"new_recovered\", IntegerType(), True),\n",
        "        StructField(\"deaths_100_cases\", DoubleType(), True),\n",
        "        StructField(\"recovered_100_cases\", DoubleType(), True),\n",
        "        StructField(\"deaths_100_recovered\", DoubleType(), True),\n",
        "        StructField(\"num_countries\", IntegerType(), True)\n",
        "    ]),\n",
        "    \"full_grouped.csv\": StructType([\n",
        "        StructField(\"date\", DateType(), True),\n",
        "        StructField(\"country_region\", StringType(), True),\n",
        "        StructField(\"confirmed\", IntegerType(), True),\n",
        "        StructField(\"deaths\", IntegerType(), True),\n",
        "        StructField(\"recovered\", IntegerType(), True),\n",
        "        StructField(\"active\", IntegerType(), True),\n",
        "        StructField(\"new_cases\", IntegerType(), True),\n",
        "        StructField(\"new_deaths\", IntegerType(), True),\n",
        "        StructField(\"new_recovered\", IntegerType(), True),\n",
        "        StructField(\"who_region\", StringType(), True)\n",
        "    ]),\n",
        "    \"usa_county_wise.csv\": StructType([\n",
        "        StructField(\"uid\", StringType(), True),\n",
        "        StructField(\"iso2\", StringType(), True),\n",
        "        StructField(\"iso3\", StringType(), True),\n",
        "        StructField(\"code3\", IntegerType(), True),\n",
        "        StructField(\"fips\", DoubleType(), True),\n",
        "        StructField(\"admin2\", StringType(), True),\n",
        "        StructField(\"province_state\", StringType(), True),\n",
        "        StructField(\"country_region\", StringType(), True),\n",
        "        StructField(\"lat\", DoubleType(), True),\n",
        "        StructField(\"long\", DoubleType(), True),\n",
        "        StructField(\"combined_key\", StringType(), True),\n",
        "        StructField(\"date\", DateType(), True),\n",
        "        StructField(\"confirmed\", IntegerType(), True),\n",
        "        StructField(\"deaths\", IntegerType(), True)\n",
        "    ]),\n",
        "    \"worldometer_data.csv\": StructType([\n",
        "        StructField(\"country_region\", StringType(), True),\n",
        "        StructField(\"continent\", StringType(), True),\n",
        "        StructField(\"population\", IntegerType(), True),\n",
        "        StructField(\"total_cases\", IntegerType(), True),\n",
        "        StructField(\"new_cases\", IntegerType(), True),\n",
        "        StructField(\"total_deaths\", IntegerType(), True),\n",
        "        StructField(\"new_deaths\", IntegerType(), True),\n",
        "        StructField(\"total_recovered\", IntegerType(), True),\n",
        "        StructField(\"new_recovered\", IntegerType(), True),\n",
        "        StructField(\"active_cases\", IntegerType(), True),\n",
        "        StructField(\"serious_critical\", IntegerType(), True),\n",
        "        StructField(\"tot_cases_1m_pop\", DoubleType(), True),\n",
        "        StructField(\"deaths_1m_pop\", DoubleType(), True),\n",
        "        StructField(\"total_tests\", IntegerType(), True),\n",
        "        StructField(\"tests_1m_pop\", DoubleType(), True),\n",
        "        StructField(\"who_region\", StringType(), True)\n",
        "    ])\n",
        "    # Agrega esquemas para más archivos CSV si es necesario\n",
        "}"
      ],
      "metadata": {
        "id": "nlpURx8bUHoh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Job de Ingesta"
      ],
      "metadata": {
        "id": "xHnq7qaaVIyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class csvReader:\n",
        "    def __init__(self):\n",
        "        self.spark = SparkSession.builder.getOrCreate()\n",
        "        self.dfs = {}\n",
        "\n",
        "    def read_csv_files(self, file_paths, schema):\n",
        "\n",
        "        #Importante, aquí se usan los Schemas establecidos para cada .Csv\n",
        "        for path in file_paths:\n",
        "            df = self.spark.read.csv(\"data/\" + path, schema=schema[path], header=True)\n",
        "            self.dfs[path] = df\n",
        "        print(\"Se aplicaron todos los Schemas descritos en structureSchema.py \")\n",
        "\n",
        "        print(\"Ingesta Completada se lecturan todos los archivos .csv\")\n",
        "\n",
        "        return self.dfs"
      ],
      "metadata": {
        "id": "-GnUH0jaVHhg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Job de Limpieza de datos"
      ],
      "metadata": {
        "id": "F96j_EChVO9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataCleaner:\n",
        "    def __init__(self):\n",
        "        self.df = {}\n",
        "\n",
        "    def clean_data(self, df: DataFrame) -> DataFrame:\n",
        "        string_columns = [column_name for column_name, data_type in df.dtypes if data_type == 'string']\n",
        "        # Se realiza limpieza de datos para reemplazar valores nulos por string vacio\n",
        "        for col_name in string_columns:\n",
        "            df = df.withColumn(col_name, when(col(col_name).isNull(), \"\").otherwise(col(col_name)))\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "Oze2ka2sUKkY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Job de Escritura y Actualización de datos\n"
      ],
      "metadata": {
        "id": "TmkqIGN5VTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataProcess:\n",
        "    def __init__(self):\n",
        "        self.spark = SparkSession.builder.getOrCreate()\n",
        "        self.df = {}\n",
        "\n",
        "    def process_and_write_data(self,path, df: DataFrame) -> DataFrame:\n",
        "        # Escribir el DataFrame en formato Parquet en modo 'append'\n",
        "        try:\n",
        "            if os.path.exists(path):\n",
        "                # Si el archivo Parquet de salida ya existe, lee los datos existentes como DataFrame\n",
        "                df_parquet = spark.read.parquet(path)\n",
        "                # Encuentra las novedades (diferencias) entre los datos CSV y los datos Parquet existentes\n",
        "                df_novedades = df.subtract(df_parquet)\n",
        "\n",
        "                # Agrega nuevos registros en formato Parquet\n",
        "                df_novedades.write.mode(\"append\").parquet(path)\n",
        "            else:\n",
        "                df.write.mode(\"overwrite\").parquet(path)\n",
        "\n",
        "        except Exception as e:\n",
        "        # Captura cualquier excepción y muestra información de error adicional.\n",
        "            print(\"Error al guardar los datos en formato Parquet:\")\n",
        "            print(str(e))"
      ],
      "metadata": {
        "id": "VG8N7_eBUNXx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataPipeline:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.csv_reader = csvReader()\n",
        "        self.data_cleaner = dataCleaner()\n",
        "        self.data_processor = dataProcess()\n",
        "        # self.file_monitor = FileMonitor()\n",
        "\n",
        "    def run(self):\n",
        "        # Quitamos el .csv de los nombres de archivos para definir el nombra de salida en parquet\n",
        "        output_path = [file_path.split(\".csv\")[0] for file_path in file_paths]\n",
        "\n",
        "        # Ingesta y guardado en dataframes\n",
        "        dfs = self.csv_reader.read_csv_files(file_paths, schema)\n",
        "\n",
        "        #Limpieza de datos usando la clase dataClean para eliminar nulos en campos String y reemplazar por string vacios ''\n",
        "        df_cleaned = {}\n",
        "        print('\\n################## INICIANDO LIMPIEZA DE DATOS   ##################')\n",
        "        for dataframe_name in dfs.keys():\n",
        "            df_cleaned[dataframe_name] = self.data_cleaner.clean_data(dfs[dataframe_name])\n",
        "            print(\"Limpieza de Datos completada para el dataframe \"+ dataframe_name)\n",
        "\n",
        "\n",
        "        # Control de Datos y Offset / option Write Append si son nuevos registros / Overwrite si no existe la tabla en parquet\n",
        "        print('\\n################## ACTUALIZANDO DATOS EN PARQUET ##################')\n",
        "        for dataframe_name in df_cleaned.keys():\n",
        "            self.data_processor.process_and_write_data(\"output/\"+dataframe_name.split(\".csv\")[0]+\".parquet\",df_cleaned[dataframe_name])\n",
        "            print(\"Agregando nuevos registros encontrados en \" + dataframe_name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Crear una instancia de la clase DataPipeline y ejecutar el flujo de datos\n",
        "    data_pipeline = dataPipeline()\n",
        "    data_pipeline.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOYTKoLDUPVv",
        "outputId": "cb833fee-7ef6-4c92-f726-24e0a5ce7442"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se aplicaron todos los Schemas descritos en structureSchema.py \n",
            "Ingesta Completada se lecturan todos los archivos .csv\n",
            "\n",
            "################## INICIANDO LIMPIEZA DE DATOS   ##################\n",
            "Limpieza de Datos completada para el dataframe country_wise_latest.csv\n",
            "Limpieza de Datos completada para el dataframe covid_19_clean_complete.csv\n",
            "Limpieza de Datos completada para el dataframe day_wise.csv\n",
            "Limpieza de Datos completada para el dataframe full_grouped.csv\n",
            "Limpieza de Datos completada para el dataframe usa_county_wise.csv\n",
            "Limpieza de Datos completada para el dataframe worldometer_data.csv\n",
            "\n",
            "################## ACTUALIZANDO DATOS EN PARQUET ##################\n",
            "Agregando nuevos registros encontrados en country_wise_latest.csv\n",
            "Agregando nuevos registros encontrados en covid_19_clean_complete.csv\n",
            "Agregando nuevos registros encontrados en day_wise.csv\n",
            "Agregando nuevos registros encontrados en full_grouped.csv\n",
            "Agregando nuevos registros encontrados en usa_county_wise.csv\n",
            "Agregando nuevos registros encontrados en worldometer_data.csv\n"
          ]
        }
      ]
    }
  ]
}